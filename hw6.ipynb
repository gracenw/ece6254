{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced69689",
   "metadata": {},
   "source": [
    "## problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f705652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_SVs(kernel_type, C_opt, deg = 1, gamma_opt=0):\n",
    "    if kernel_type == 'poly':\n",
    "        clf = svm.SVC(C=C_opt,kernel='poly',degree=deg,coef0=1.0)\n",
    "    elif kernel_type == 'rbf':\n",
    "        clf = svm.SVC(C=C_opt,kernel='rbf',gamma=gamma_opt)\n",
    "\n",
    "    clf.fit(X_train,y_train)\n",
    "\n",
    "    SVs = clf.support_vectors_\n",
    "    alpha = clf.dual_coef_\n",
    "\n",
    "    xi = 1 - np.squeeze(np.sign(alpha))*clf.decision_function(SVs)\n",
    "\n",
    "    origidx = clf.support_\n",
    "\n",
    "    sortidx = np.argsort(xi)\n",
    "\n",
    "    topSVs = SVs[sortidx[-16:],:]\n",
    "    topSVlabels = y_train[origidx[sortidx[-16:]]]\n",
    "\n",
    "    f, axarr = plt.subplots(4, 4, figsize = (10, 10))\n",
    "    ii=0\n",
    "    jj=0\n",
    "    for idx in range(16):\n",
    "        axarr[ii, jj].imshow(topSVs[idx].reshape((28,28)), cmap='gray')\n",
    "        axarr[ii, jj].axis('off')\n",
    "        axarr[ii, jj].set_title('{label}'.format(label=int(topSVlabels[idx])))\n",
    "        ii = ii + 1\n",
    "        if ii==4:\n",
    "            ii = 0\n",
    "            jj = jj+1\n",
    "\n",
    "    filename = ''\n",
    "    if kernel_type == 'poly':\n",
    "        if deg == 1:\n",
    "            filename += 'lin'\n",
    "        elif deg == 2:\n",
    "            filename += 'quad'\n",
    "    elif kernel_type == 'rbf':\n",
    "        filename += 'rbf'\n",
    "\n",
    "    filename += 'SVs.png'\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db75684",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", version=1, as_frame=False)\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "# View the jth image\n",
    "# j = 1\n",
    "# plt.title('The jth image is a {label}'.format(label=int(y[j])))\n",
    "# plt.imshow(X[j].reshape((28,28)), cmap='gray')\n",
    "# plt.show()\n",
    "\n",
    "X4 = X[y==4,:]\n",
    "X9 = X[y==9,:]\n",
    "y4 = np.full(X4.shape[0], 4)\n",
    "y9 = np.full(X9.shape[0], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training/validation/testing sets for each class\n",
    "_X4, X_test4, _y4, y_test4 = train_test_split(X4, y4, train_size = 4000)\n",
    "X_train4, X_val4, y_train4, y_val4 = train_test_split(_X4, _y4, train_size = 2000)\n",
    "_X9, X_test9, _y9, y_test9 = train_test_split(X9, y9, train_size = 4000)\n",
    "X_train9, X_val9, y_train9, y_val9 = train_test_split(_X9, _y9, train_size = 2000)\n",
    "\n",
    "# Vertical stack to have one X vector each for train/validate/test\n",
    "X_train = np.vstack((X_train4, X_train9))\n",
    "X_val = np.vstack((X_val4, X_val9))\n",
    "X_test = np.vstack((X_test4, X_test9))\n",
    "\n",
    "# Combine to have one y vector each for train/validate/test\n",
    "y_train = np.append(y_train4, y_train9)\n",
    "y_val = np.append(y_val4, y_val9)\n",
    "y_test = np.append(y_test4, y_test9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0367f80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Cvec = np.logspace(-3, 3, 20)\n",
    "\n",
    "for deg in [1, 2]:\n",
    "    print(\"Training SVM with polynomial kernel, deg = \" + str(deg))\n",
    "    Pe_train = []   # Probability of error for training set\n",
    "    Pe_val = []     # Probability of error for validation set\n",
    "    Pe_test = []    # Probability of error for testing set\n",
    "    numSvs = []     # Number of support vectors\n",
    "\n",
    "    for Cval in Cvec:\n",
    "        print('Testing Cval = ' + str(Cval))\n",
    "        \n",
    "        # Train SVM w/ polynomial kernel using Cval and deg\n",
    "        clf = svm.SVC(C = Cval, kernel = 'poly', degree = deg)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Compute the probability of error on all 3 sets and store\n",
    "        Pe_train.append(1 - clf.score(X_train, y_train))\n",
    "        Pe_val.append(1 - clf.score(X_val, y_val))\n",
    "        Pe_test.append(1 - clf.score(X_test, y_test))\n",
    "\n",
    "        # Track the number of support vectors\n",
    "        numSvs.append(clf.support_vectors_.shape[0])\n",
    "\n",
    "    # Based on the validation set, set the value for C\n",
    "    Cval_opt = Cvec[np.argmin(Pe_val)]\n",
    "    print('Optimal value of C based on validation set: ' + str(Cval_opt))\n",
    "    print('Train error: ' + str(Pe_train[np.argmin(Pe_val)]))\n",
    "    print('Test error: ' + str(Pe_test[np.argmin(Pe_val)]))\n",
    "    print('Number of support vectors: ' + str(numSvs[np.argmin(Pe_val)]))\n",
    "\n",
    "    # Visualize how training/validation/testing error and # of SVs changes with C\n",
    "    with np.printoptions(precision = 4):\n",
    "        print('Pe_train: ', np.array(Pe_train))\n",
    "        print('Pe_val: ', np.array(Pe_val))\n",
    "        print('Pe_test: ', np.array(Pe_test))\n",
    "    print('numSvs: ', numSvs)\n",
    "\n",
    "    # Display the SVs that are hardest to classify\n",
    "    display_SVs('poly', Cval_opt, deg = deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pe_train = []   # Probability of error for training set\n",
    "Pe_val = []     # Probability of error for validation set\n",
    "Pe_test = []    # Probability of error for testing set\n",
    "numSvs = []     # Number of support vectors\n",
    "\n",
    "Gammavec = np.logspace(-9, 1, 20)\n",
    "\n",
    "for Gammaval in Gammavec:\n",
    "    print('Testing Gammaval = ' + str(Gammaval))\n",
    "    \n",
    "    # Train SVM w/ rbf kernel using C = 10 and Gammaval\n",
    "    clf = svm.SVC(C = 10, kernel = 'rbf', gamma = Gammaval)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Compute the probability of error on all 3 sets and store\n",
    "    Pe_train.append(1 - clf.score(X_train, y_train))\n",
    "    Pe_val.append(1 - clf.score(X_val, y_val))\n",
    "    Pe_test.append(1 - clf.score(X_test, y_test))\n",
    "\n",
    "    # Track the number of support vectors\n",
    "    numSvs.append(clf.support_vectors_.shape[0])\n",
    "\n",
    "Gammaval_opt = Gammavec[np.argmin(Pe_val)]\n",
    "print('Optimal value of Gamma based on validation set: ' + str(Gammaval_opt))\n",
    "print('Train error: ' + str(Pe_train[np.argmin(Pe_val)]))\n",
    "print('Test error: ' + str(Pe_test[np.argmin(Pe_val)]))\n",
    "print('Number of support vectors: ' + str(numSvs[np.argmin(Pe_val)]))\n",
    "\n",
    "with np.printoptions(precision = 4):\n",
    "    print('Pe_train: ', np.array(Pe_train))\n",
    "    print('Pe_val: ', np.array(Pe_val))\n",
    "    print('Pe_test: ', np.array(Pe_test))\n",
    "print('numSvs: ', numSvs)\n",
    "\n",
    "display_SVs('rbf', 10, gamma_opt = Gammaval_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da33b3",
   "metadata": {},
   "source": [
    "## problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05aea88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training and testing datasets\n",
    "np.random.seed(2022)\n",
    "\n",
    "n = 100\n",
    "\n",
    "X_train = np.random.rand(n)\n",
    "y_train = np.sin(9 * X_train) + np.sqrt(1 / 3.0) * np.random.randn(n)\n",
    "\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "y_test = np.sin(9 * X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# krr class for simplicity\n",
    "class krr:    \n",
    "    def __init__(self, _X_train, _y_train):\n",
    "        self.X_train = _X_train\n",
    "        self.y_train = _y_train.reshape(-1, 1)\n",
    "        \n",
    "    def predict(self, l, g, X_test, y_test):\n",
    "        n = self.X_train.shape[0]\n",
    "        \n",
    "        # Generate K matrix\n",
    "        K = np.empty((n, n))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                # radial basis function\n",
    "                K[i, j] = exp(-g * (self.X_train[i] - self.X_train[j]) ** 2)\n",
    "        \n",
    "        # Predict y for each X sample in X_test\n",
    "        y_pred = np.empty(n)\n",
    "        for i in range(n):\n",
    "            k_x = np.empty(n)\n",
    "            for j in range(n):\n",
    "                # radial basis function\n",
    "                k_x[j] = exp(-g * (self.X_train[j] - X_test[i]) ** 2)\n",
    "            y_pred[i] = self.y_train.T @ np.linalg.inv(K + l * np.identity(n)) @ k_x.reshape(1, -1).T\n",
    "        \n",
    "        # For optimization of parameters calculate MSE\n",
    "        mse = (np.square(y_pred - y_test)).mean()\n",
    "\n",
    "        return y_pred, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca983ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernelized ridge regression with radial basis function\n",
    "Pe_test = []\n",
    "\n",
    "Lvec = np.logspace(-3, 3, 20)\n",
    "Gvec = np.logspace(-3, 3, 20)\n",
    "\n",
    "reg = krr(X_train, y_train)\n",
    "\n",
    "# Predict y values for each value of gamma\n",
    "for g in Gvec:\n",
    "    y_pred, mse = reg.predict(1.0, g, X_test, y_test)\n",
    "    Pe_test.append(mse)\n",
    "\n",
    "Gopt = Gvec[np.argmin(Pe_test)]\n",
    "print('Optimal gamma value: ', Gopt)\n",
    "\n",
    "Pe_test = []\n",
    "\n",
    "# Predict y values for each value of lambda, given optimal gamma\n",
    "for l in Lvec:\n",
    "    y_pred, mse = reg.predict(l, Gopt, X_test, y_test)\n",
    "    Pe_test.append(mse)\n",
    "\n",
    "Lopt = Lvec[np.argmin(Pe_test)]\n",
    "print('Optimal lambda value: ', Lopt)\n",
    "\n",
    "# Get predicted y values and MSE for optimized parameters\n",
    "y_pred, mse = reg.predict(Lopt, Gopt, X_test, y_test)\n",
    "\n",
    "print('Test MSE: ', mse)\n",
    "\n",
    "plt.scatter(X_test, y_pred, label = 'Predicted');\n",
    "plt.scatter(X_test, y_test, label = 'Actual');\n",
    "plt.legend();\n",
    "plt.xlabel('X');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernelized support vector regression with radial basis function\n",
    "Pe_test = []\n",
    "\n",
    "Cvec = np.logspace(-3, 3, 20)\n",
    "Gvec = np.logspace(-2, 2, 20)\n",
    "Evec = np.logspace(-6, -1, 20)\n",
    "\n",
    "# Fit and predict SVR for each cost value\n",
    "for C in Cvec:\n",
    "    reg = SVR(C = C, epsilon = 0.1, kernel = 'rbf', gamma = 5.0)\n",
    "    reg.fit(X_train.reshape(-1, 1), y_train)\n",
    "    Pe_test.append(1 - reg.score(X_test.reshape(-1, 1), y_test.reshape(-1, 1)))\n",
    "\n",
    "Copt = Cvec[np.argmin(Pe_test)]\n",
    "print('Optimal cost value: ', Copt)\n",
    "\n",
    "Pe_test = []\n",
    "\n",
    "# Fit and predict SVR for each gamma value, given optimal cost\n",
    "for g in Gvec:\n",
    "    reg = SVR(C = Copt, epsilon = 0.1, kernel = 'rbf', gamma = g)\n",
    "    reg.fit(X_train.reshape(-1, 1), y_train)\n",
    "    Pe_test.append(1 - reg.score(X_test.reshape(-1, 1), y_test.reshape(-1, 1)))\n",
    "\n",
    "Gopt = Gvec[np.argmin(Pe_test)]\n",
    "print('Optimal gamma value: ',  Gopt)\n",
    "\n",
    "Pe_test = []\n",
    "\n",
    "# Fit and predict SVR for each epsilon value, given optimal cost and gamma\n",
    "for e in Evec:\n",
    "    reg = SVR(C = Copt, epsilon = e, kernel = 'rbf', gamma = Gopt)\n",
    "    reg.fit(X_train.reshape(-1, 1), y_train)\n",
    "    Pe_test.append(1 - reg.score(X_test.reshape(-1, 1), y_test.reshape(-1, 1)))\n",
    "\n",
    "Eopt = Evec[np.argmin(Pe_test)]\n",
    "print('Optimal epsilon value: ', Eopt)\n",
    "\n",
    "# Fit and predict SVR using all three optimized parameters\n",
    "reg = SVR(C = Copt, epsilon = Eopt, kernel = 'rbf', gamma = Gopt)\n",
    "reg.fit(X_train.reshape(-1, 1), y_train)\n",
    "y_pred = reg.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "print('Test error: ', 1 - reg.score(X_test.reshape(-1, 1), y_test.reshape(-1, 1)))\n",
    "\n",
    "plt.scatter(X_test, y_pred, label = 'Predicted');\n",
    "plt.scatter(X_test, y_test, label = 'Actual');\n",
    "plt.legend();\n",
    "plt.xlabel('X');\n",
    "plt.ylabel('Y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b0c56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
